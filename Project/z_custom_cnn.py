# -*- coding: utf-8 -*-
"""ChallengeCustomCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AWLzB-uVg1IrI-YHIBifbCZf5s3iOFDZ
"""

from google.colab import drive
drive.mount('/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd "/gdrive/MyDrive/ChallengeANN"

import tensorflow as tf
import numpy as np
import os
import random
import pandas as pd
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
from PIL import Image
import shutil, random, json
from sklearn.utils import compute_class_weight
from datetime import datetime

tfk = tf.keras
tfkl = tf.keras.layers
print(tf.__version__)

# Random seed for reproducibility
seed = 42

random.seed(seed)
os.environ['PYTHONHASHSEED'] = str(seed)
np.random.seed(seed)
tf.random.set_seed(seed)
tf.compat.v1.set_random_seed(seed)

import warnings
import logging

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=Warning)
tf.get_logger().setLevel('INFO')
tf.autograph.set_verbosity(0)

tf.get_logger().setLevel(logging.ERROR)
tf.get_logger().setLevel('ERROR')
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)

import os.path
from os import path

dataset_dir = "training_data_final"

# Load the dataset to be used for classification
if not os.path.exists(dataset_dir):
    !unzip Training_dataset_homework1.zip

labels = ['Species1',
          'Species2',
          'Species3',
          'Species4',
          'Species5',
          'Species6',
          'Species7',
          'Species8']

# Split dataset
# check if the tmp folder exist
splitPercentage = 0.8

random.seed(seed)
path = 'temp_split'
if os.path.exists(path):
    shutil.rmtree(path)
if not os.path.exists(path):
    os.mkdir(path)
if not os.path.exists(path + '/training'):
    os.mkdir(path + '/training')
if not os.path.exists(path + '/validation'):
    os.mkdir(path + '/validation')

# Source path
source = "training_data_final"

# Destination path
dest_train = path + '/training'
dest_valid = path + '/validation'

# create train and validation into the tmp folder
for folder in os.listdir(source):
    if not os.path.exists(dest_train + '/' + folder):
        os.mkdir(dest_train + '/' + folder)
    if not os.path.exists(dest_valid + '/' + folder):
        os.mkdir(dest_valid + '/' + folder)

    cl = source + '/' + folder  # create path of the class
    files = os.listdir(cl)  # list of files for the class
    #folder
    random.shuffle(files)
    # create training set randomly
    for i in range(int(len(files) * splitPercentage)):
        dest = shutil.copy(cl + '/' + files[i],
                           dest_train + '/' + folder + '/' + files[i])  # copy an image in the training set
        #print("Training: ", files[i])
    # create validation set randomly
    for j in range(i + 1, len(files)):
        dest = shutil.copy(cl + '/' + files[j],
                           dest_valid + '/' + folder + '/' + files[j])  # copy an image in the validation set
        #print("Validation: ", files[j])

from tensorflow.keras.preprocessing.image import ImageDataGenerator

batch_size = 16

# Create an instance of ImageDataGenerator with Data Augmentation
train_data_gen = ImageDataGenerator(rotation_range=360,
                                    height_shift_range=0.2,
                                    width_shift_range=0.2,
                                    zoom_range=0.4,
                                    brightness_range=(0.5, 1.4),
                                    shear_range=25,
                                    horizontal_flip=True,
                                    vertical_flip=True,
                                    fill_mode='reflect',
                                    rescale= 1. / 255)  # rescale value is multiplied to the image

valid_data_gen = ImageDataGenerator(rescale=1. / 255)

training_dir = 'temp_split/training'
validation_dir = 'temp_split/validation'

# Obtain a data generator with the 'ImageDataGenerator.flow_from_directory' method
training_augmentated = train_data_gen.flow_from_directory(directory=training_dir,
                                                          target_size=(96, 96),
                                                          color_mode='rgb',
                                                          classes=labels,
                                                          class_mode='categorical',
                                                          batch_size=batch_size,
                                                          shuffle=True,
                                                          seed=seed)

# Using also the validation augmented will affect the accuracy value, showing a higher accuracy
validation = valid_data_gen.flow_from_directory(directory=validation_dir,
                                                            target_size=(96, 96),
                                                            color_mode='rgb',
                                                            classes=labels,
                                                            class_mode='categorical',
                                                            batch_size=batch_size,
                                                            shuffle=True,
                                                            seed=seed)

# Compute class weights
class_weights = compute_class_weight(class_weight="balanced",
                                     classes=np.unique(training_augmentated.classes),
                                     y=training_augmentated.classes)
class_weights = dict(zip(np.unique(training_augmentated.classes), class_weights))

input_shape = (96, 96, 3)
epochs = 200

def build_model(input_shape):

    # Build the neural network layer by layer
    input_layer = tfkl.Input(shape=input_shape, name='input_layer')

    x = tfkl.Conv2D(
        filters=128,
        kernel_size=3,
        padding = 'same',
        activation = 'relu',
        kernel_initializer = tfk.initializers.HeUniform(seed)
    )(input_layer)
    x = tfkl.MaxPooling2D()(x)

    x = tfkl.Conv2D(
        filters=256,
        kernel_size=3,
        padding = 'same',
        activation = 'relu',
        kernel_initializer = tfk.initializers.HeUniform(seed)
    )(x)
    x = tfkl.MaxPooling2D()(x)

    x = tfkl.Conv2D(
        filters=512,
        kernel_size=3,
        padding = 'same',
        activation = 'relu',
        kernel_initializer = tfk.initializers.HeUniform(seed)
    )(x)
    x = tfkl.MaxPooling2D()(x)

    x = tfkl.Conv2D(
        filters=1024,
        kernel_size=3,
        padding = 'same',
        activation = 'relu',
        kernel_initializer = tfk.initializers.HeUniform(seed)
    )(x)
    x = tfkl.MaxPooling2D()(x)

    x = tfkl.Conv2D(
        filters=2048,
        kernel_size=3,
        padding = 'same',
        activation = 'relu',
        kernel_initializer = tfk.initializers.HeUniform(seed)
    )(x)
    x = tfkl.MaxPooling2D()(x)

    x = tfkl.Flatten(name='Flatten')(x)
    x = tfkl.Dropout(0.35, seed=seed)(x)
    x = tfkl.Dense(units=512, name='Classifier', kernel_initializer=tfk.initializers.HeUniform(seed), activation='relu')(x)
    x = tfkl.Dropout(0.35, seed=seed)(x)
    output_layer = tfkl.Dense(units=len(labels), activation='softmax', kernel_initializer=tfk.initializers.GlorotUniform(seed), name='output_layer')(x)

    # Connect input and output through the Model class
    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')

    # Compile the model
    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate=1e-4), metrics='accuracy')
    
    # Return the model
    return model

# Utility function to create folders and callbacks for training
def create_folders_and_callbacks(model_name):

  exps_dir = os.path.join('data_augmentation_experiments')
  if not os.path.exists(exps_dir):
      os.makedirs(exps_dir)

  now = datetime.now().strftime('%b%d_%H-%M-%S')

  exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))
  if not os.path.exists(exp_dir):
      os.makedirs(exp_dir)
      
  callbacks = []

  # Early Stopping
  # --------------
  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)
  callbacks.append(es_callback)

  return callbacks

# Build model
model = build_model(input_shape)
model.summary()

# Create folders and callbacks and fit
callbacks = create_folders_and_callbacks(model_name='CNN')

# Train the model
history_model = model.fit(
    x=training_augmentated,
    batch_size=batch_size,
    epochs=epochs,
    shuffle=True,
    validation_data=validation,
    callbacks=callbacks,
    class_weight=class_weights
).history

model.save("data_augmentation_experiments/BestCustomCNN_class_weights_split_95")

# Plot the training
plt.figure(figsize=(15,5))
plt.plot(history_model['loss'], label='Training', alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history_model['val_loss'], label='Validation', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(history_model['accuracy'], label='Training', alpha=.8, color='#ff7f0e', linestyle='--')
plt.plot(history_model['val_accuracy'], label='Validation', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()